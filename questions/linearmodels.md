# Questions and Answers to Linear Models.

1. [Jupyter notebook KPNA2 gene analysis] 
In the Jupyter notebook example, an interaction term between grade and node removal is tested, and the model reports a p-value for it. Could you explain more about interaction terms? How is it included in the regression model? Why is it included in the regression instead of doing a separate test to examine if there is an interaction between the grade and node?

1. Is there a p-value associated with a statistical model we build so it can be used to regret the null hypothesis (the data does not fit to the statistical model) ?
   > I think chapter 10.6 in Downey gives a really thorough explanation how to do that. In brief, we are interested in either:
   > a) what is the probablity of obtaining a slope as extreme or more under Ho;
   > b) the probablity of obtaining a slope smaller/larger than 0 (for positive/negative correlation) under our sampling distribution. That would roughly translate to estimating how probable it is that our data actually comes from an uncorrelated dataset, but for a visualization with actual sampling distributions, you can check out fig. 10.4 in Downey.
   > However, the slope is not exactly our test statistic. It is R2 (coefficient of determination, you surely know about it, but if you want a reminder what it actually stands for, I think the formulas on wikipedia are quite useful: https://en.wikipedia.org/wiki/Coefficient_of_determination (Links to an external site.)). That allows us to build an actual statistical model, as R2 is connected to how much the obtained fit to the data differs from what we would expect just based on variance in dependent variable. In other words, how much our model explains the data better than random model, which we are exactly interested in. Then, R2 is directly related to the Pearson correlation coefficient (R, r or ρ), as the R in R2 actually stand for the Pearson correlation coefficient. So R2 is indifferent to whether the correlation is positive or negative, and that way testing for R2 is equalivent to two-sided test for R, while if you were interested in one-sided test, you would just use R as the test statistic.

1. [Lecture video 1:59]
   Why do we use two types of statistical tests for the creation of linear models, if both of them use the same statistical methods and residual calculations (eI)? Does one have an advantage over the other calculation, as the video says both calculations are used to differentiate between diseased and healthy patients? What is the meaning of a discreet function? [video 5:01]
   > The two types of tests are used because they apply to different types of data. It is true that both of them use the same equation/calculations, but one applies to continuous data (x as a function of y, so for example age vs height) and results in a regression line with a slope (*y=ax+b*). The other applies to categorical data (so different categories like sick or healthy) and results in lines without slopes (*y=b* or *y=a+b*). This is also what discrete functions mean, individual values as opposed to continuous in a particular range. So we are basically using the techniques behind linear regression to formulate a t-test between categories. I hope I understood your question correctly! [Here's a nice video that explains this further!](https://www.youtube.com/watch?v=NF5_btOaCig&t=610s&ab_channel=StatQuestwithJoshStarmer)

1. When modeling the p-value for an interaction between two variables as in the tumour size and grade and their effect on the expression on KPNA2 from the jupyter notebook. How are the effects of each variable convoluted from each other in the modeling? In the last cell of the notebook we see a very significant relationship between grade and expression and not so for size and expression. For the combination of grade and size we see a significant effect on the expression. My question is how do we know that the size is not just "piggybacking" in the very significant relationship between expression and grade so that the combination size:grade is still significant? 

1. If the span in the smoothed regression is very large or very small would the variance of the resulting best fit line be too low or too high, meaning that the data will be oversmoothed or shift too rapidly respectively and thus present errors in the regression, is it possible to check these errors via a q-q plot of the residuals versus a normal distribution with mean and variance?

1. Could you explain more about the relationship between dependent variable and explanatory variables? In my world they should be synonyms, but clearly there is a different. 

1. Considering the online lecture (10:45), if we have three parameters correlating with each other in one way (positive or negative), but let's say, in reality, only A is interacting with B and C independently (there is no interaction between B and C but we see a similar correlation), how can the undirect correlation in this system be identified/eliminated? Could it be done using the available data? 

1. Is there a statistic that can provide hints towards which kind of model could explain our data? Or we find this out empirically by trying out several models.

1. Chapter 10 Linear regression in Downey seem to imply that the residuals should be normally distributed but I was wondering about the variables, if they need to be normally distributed before being able to run a linear regression?

1. Downey, Chapter 10.5 (Goodness of fit)
I do not completely understand how Std(res) and Std(ys) are calculated and why Std(res) would be a better estimate of the quality of the linear model in comparison to Std(ys).