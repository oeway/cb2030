{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Pathway analysis and Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "# Pathway analysis\n",
    "\n",
    "Let's pick up from where we left in the last lab. By the end of the lab you found interesting genes that were **differentially expressed** between two **clinically relevant** conditions.\n",
    "\n",
    "(Note: the selection of relevant clinical conditions is even more important in this lab, if you haven't spent some time on it previously, take the time now.)\n",
    "\n",
    "You've also learned that one way to make more sense of these results is by performing **pathway analysis**, so let's try that.\n",
    "\n",
    "We, as always, start by importing relevant libraries and loading the data.\n",
    "For pathway analysis, we will be mostly working with the [gseapy](https://github.com/ostrokach/gseapy) library, which is mostly a python wrapper for GSEA and Enrichr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gseapy as gp\n",
    "from gseapy.plot import gseaplot\n",
    "import qvalue\n",
    "\n",
    "from ipywidgets import interact, interact_manual\n",
    "from ipywidgets import IntSlider, FloatSlider, Dropdown, Text\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "interact_enrich=interact_manual.options(manual_name=\"Enrichment analysis\")\n",
    "interact_plot=interact_manual.options(manual_name=\"Plot\")\n",
    "interact_calc=interact_manual.options(manual_name=\"Calculate tests\")\n",
    "\n",
    "interact_gen=interact_manual.options(manual_name=\"Initialize data\")\n",
    "interact_SVM=interact_manual.options(manual_name=\"Train SVM\")\n",
    "\n",
    "clinical_data = pd.read_csv('../data/brca_clin.tsv.gz', sep ='\\t', index_col=2)\n",
    "clinical_data = clinical_data.iloc[4:,1:]\n",
    "expression_data = pd.read_csv('../data/brca.tsv.gz', sep ='\\t', index_col=1)\n",
    "expression_data = expression_data.iloc[:,2:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Over Representation Analysis\n",
    "\n",
    "We beggin with Enrichr, which performs **Over Representation Analysis (ORA)**.\n",
    "You have learned that ORA compares two set of genes and calculates **how likely would their overlap occur by random**.\n",
    "\n",
    "We will call the first set of genes the 'query set', and they will be the genes found to be *most significantly differentially expressed*, but you will get a chance to define what that means.\n",
    "Here, you can separate the query genes from the rest by thresholding on either the *p-value* or *q-value*, and the log2 fold change.\n",
    "\n",
    "The second gene set on the test is the **pathway**, and you will be able to select one of many pathway databases available online. Each database divides the genome into sets according to different criteria.\n",
    "\n",
    "The test will then compare the overlap between your query set of genes and all the pathways on the database, returning the corresponding statistics.\n",
    "\n",
    "Interactive fields:\n",
    "* **Pathway_DB**: Your choice of pathway database.\n",
    "* **Statistic**: Which statistic to use for thresholding, p or q-value.\n",
    "* **Threshold**: The statistical threshold.\n",
    "* **Lo2FC_threshold**: The log2 fold change threshold.\n",
    "\n",
    "Use the interactive app below to answer the following questions:\n",
    "\n",
    "### Questions\n",
    "\n",
    "* 1.1 Imagine an organism that has 10 genes, and a pathway that is composed of all those genes. You run an experiment, test all 10 genes, and find 3 genes that are differentially expressed and belong to that pathway. What would be the p-value for enrichment on that pathway according to ORA?\n",
    "* 1.2 What difference does it make to threshold the list by p-value or q-value? (Hint: think of the FDR of the resulting enriched pathways)\n",
    "* 1.3 Did you choose to threshold by the log2 fold change? Why?\n",
    "* 1.4 Did you relate your choice of thresholds to the nature of the clinical groups you are studying?\n",
    "* 1.5 After finding good parameters, give a brief interpretation of the pathways that are significantly enriched for this condition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_col(col):\n",
    "    fc = np.mean(col[index1]) - np.mean(col[index2])\n",
    "    p_val = sp.stats.ttest_ind(col[index1].dropna(), col[index2].dropna()).pvalue\n",
    "    return [p_val, fc]\n",
    "\n",
    "def differential_test(clinical_df, expression_df, separator, cond1, cond2):\n",
    "    global index1, index2\n",
    "    try:\n",
    "        group1 = clinical_df[separator] == cond1\n",
    "        index1 = clinical_df[group1].index\n",
    "        group2 = clinical_df[separator] == cond2\n",
    "        index2 = clinical_df[group2].index\n",
    "    except:\n",
    "        print('Clinical condition wrong')\n",
    "        \n",
    "    results = expression_df.apply(ttest_col, axis=0, result_type=\"expand\")\n",
    "    results = results.T\n",
    "    results.columns = ['p', 'log2fc']\n",
    "    \n",
    "    return results\n",
    "\n",
    "def differential_test_slow(clinical_df, expression_df, separator, cond1, cond2):\n",
    "    results = pd.DataFrame(columns = ['p','log2fc'])\n",
    "    try:\n",
    "        group1 = clinical_df[separator] == cond1\n",
    "        index1 = clinical_df[group1].index\n",
    "        group2 = clinical_df[separator] == cond2\n",
    "        index2 = clinical_df[group2].index\n",
    "    except:\n",
    "        print('Clinical condition wrong')\n",
    "        \n",
    "    expression1 = expression_df.loc[index1]\n",
    "    expression2 = expression_df.loc[index2]\n",
    "    \n",
    "    for gene in expression_df.columns:\n",
    "        p_val = sp.stats.ttest_ind(expression1[gene], expression2[gene]).pvalue\n",
    "        fc = np.mean(expression1[gene]) - np.mean(expression2[gene])\n",
    "        if p_val == p_val:\n",
    "            results.loc[gene,'p'] = p_val\n",
    "            results.loc[gene, 'log2fc'] = fc\n",
    "            \n",
    "    return results\n",
    "\n",
    "def plot_hist(stats, bins):\n",
    "    stats = np.array(stats)\n",
    "    plt.hist(stats, bins = bins)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interact_multiple_gene_ttest(Criteria, Group_1, Group_2):\n",
    "    global BRCA_tests\n",
    "    BRCA_tests = differential_test(clinical_data, expression_data, Criteria, Group_1, Group_2)\n",
    "    BRCA_tests = qvalue.qvalues(BRCA_tests)\n",
    "    plot_hist(BRCA_tests['p'].values, 20)\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(BRCA_tests.head(30))\n",
    "        \n",
    "def ORA(tests, threshold, log2fc_threshold, pathway_db=['KEGG_2019_Human'], stat = 'p'):\n",
    "    background=set(tests.index)\n",
    "    #gene_list = list(tests.loc[tests[stat]<threshold,stat].index)\n",
    "    gene_list = list(tests.loc[(tests[stat]<threshold) & (np.abs(tests['log2fc']) > log2fc_threshold), stat].index)\n",
    "    print('Query set size: ' + str(len(gene_list)))\n",
    "\n",
    "    output_enrichr=pd.DataFrame()\n",
    "    enr=gp.enrichr(\n",
    "                    gene_list=gene_list,\n",
    "                    gene_sets=pathway_db,\n",
    "                    background=background,\n",
    "                    outdir = None\n",
    "                )\n",
    "    results = enr.results[[\"P-value\",\"Overlap\",\"Term\"]].rename(columns={\"P-value\": \"p\"})\n",
    "    return qvalue.qvalues(results)\n",
    "\n",
    "pathway_db_choice = gp.get_library_name()\n",
    "\n",
    "        \n",
    "def interact_ORA(Pathway_DB, Statistic, Threshold, Log2FC_threshold):\n",
    "    threshold = float(Threshold)\n",
    "    log2fc_threshold = float(Log2FC_threshold)\n",
    "    results = ORA(BRCA_tests, threshold, log2fc_threshold, Pathway_DB, stat = Statistic)\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(results.head(30))\n",
    "        \n",
    "interact_calc(interact_multiple_gene_ttest, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_enrich(interact_ORA, Threshold = '5e-2' , Pathway_DB = pathway_db_choice, Statistic=['p','q'], Log2FC_threshold = Text('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Functional Class Scoring\n",
    "\n",
    "We then move on to another form of pathway analysis, dubbed **Functional Class Scoring**.\n",
    "As opposed to ORA, this type of enrichment analysis takes as an input a ranked list and a gene set, and asks **how likely is that the genes from that set are randomly distributed along the list**.\n",
    "\n",
    "Then there is no need to define a query set (!). We already have our list of genes, but we have to find a relevant way to rank it.\n",
    "\n",
    "Below you will have the option of ranking the list by either p-value, q-value or the log2 fold change.\n",
    "\n",
    "Perform the enrichment analysis as many times as you think it is needed to answer the question. The test might take a while, so be patient and make sure you have the right parameters before proceeding :)\n",
    "\n",
    "New interactive fields:\n",
    "* **Ranking**: which metric to use when ranking the results.\n",
    "\n",
    "### Questions\n",
    "\n",
    "* 2.1 Here, what is the difference between ranking genes by p-value and q-value?\n",
    "* 2.2 What are the advantages and disadvantages of ranking by log2 fold change instead (of p/q-value)?\n",
    "* 2.3 Can you think of a better way or ranking the analytes than the ones you were presented with? Cite one advantage and one disadvantage of your proposed ranking method.\n",
    "* 2.4 Again, what is your interpretation of the pathways that are significantly enriched for this clinical conditions? Did you find any difference from the ones found performing ORA? If you found any difference, why do you think these pathways are found to be enriched in one case and not the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsea(tests, stat, pathway_db = 'KEGG_2019_Human'):\n",
    "    pre_res = gp.prerank(rnk=tests[stat], \n",
    "                    gene_sets=pathway_db,\n",
    "                    processes=4,\n",
    "                    permutation_num=100, # reduce number to speed up testing\n",
    "                    outdir=None, format='png')\n",
    "    return pre_res\n",
    "\n",
    "def interact_gsea(Pathway_DB, Ranking):\n",
    "    global Results_gsea\n",
    "    Results_gsea = gsea(BRCA_tests.dropna(), pathway_db=Pathway_DB, stat = Ranking)\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(Results_gsea.res2d[['pval', 'fdr']].head(30))\n",
    "        \n",
    "interact_calc(interact_multiple_gene_ttest, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_enrich(interact_gsea, Pathway_DB = pathway_db_choice, Ranking = ['p','q','log2fc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "# 3 Support Vector Machine\n",
    "\n",
    "Ok, so now for something different.\n",
    "We continue with our dataset on breast cancer, but this time we will try to use machine learning to **predict the clinical conditions based on the gene expression data**.\n",
    "\n",
    "You again will be able two choose the two clinical groups we will use to train the model, and your choice will affect the results and performance of the model. Special attention should be paid to the number of data points in each group, which will be displayed after you select the groups.\n",
    "\n",
    "Let's train our first SVM. You've learned the merits of feature scaling (or in other terms, normalizing the input), and so we will test it.\n",
    "Below you will separate the data set into two, and train a SVM to distinguish between the two using the gene expression values of the samples. You can choose whether you'd like to rescale (normalize) the features first or not.\n",
    "\n",
    "Once you have done so, you will be presented with a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), and from that will need to devise a performance metric.\n",
    "[This Wikipedia page](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) can be helpful when choosing how to measure the performance. You might want to do that before running the SVM tests.\n",
    "\n",
    "You know the drill by now, play around with it for a while and answer the questions.\n",
    "\n",
    "Interactive fields:\n",
    "* **Rescale**: whether to apply mean and variance normalization before training the SVM.\n",
    "* **Data_split**: the fraction of the data that is left on the test set.\n",
    "* **Max_iterations**: the maximum number of iterations the SVM is allowed to perform, increase it if you are seeing convergence errors.\n",
    "\n",
    "## Questions\n",
    "\n",
    "\n",
    "* 3.1 Which clinical groups have you decided to classify using SVMs? Why did you think these are the groups which could be classified using SVMs and gene expression data? Give 1-2 arguments to justify your choice.\n",
    "* 3.2 Report the size of your two groups. Which performance metric did you choose? Give 1-2 arguments to justify your choice.\n",
    "* 3.3 How much did you chose to leave out for the test set? Justify your choice as an optimal tradeoff between the performance of your classifier and your ability to measure that performance? (What would happen to the performance and your ability to measure it if you would exclude less/more data from the training?)\n",
    "* 3.4 Did you achieve better performance by normalizing the gene expression? Provide 1-2 plausable reasons why the performance improved or did not improve.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(clinical_df, expression_df, separator, cond1, cond2):\n",
    "    try:\n",
    "        group1 = clinical_df[separator] == cond1\n",
    "        index1 = clinical_df[group1].index\n",
    "        group2 = clinical_df[separator] == cond2\n",
    "        index2 = clinical_df[group2].index\n",
    "    except:\n",
    "        print('Clinical condition wrong')\n",
    "    expression1 = expression_df.loc[index1].dropna()\n",
    "    expression2 = expression_df.loc[index2].dropna()\n",
    "    expression = pd.concat([expression1, expression2])\n",
    "    X = expression.values\n",
    "    y = np.append(np.repeat(0, len(expression1)), np.repeat(1, len(expression2)))\n",
    "    display(pd.DataFrame([len(index1),len(index2)], columns = ['Number of points'], index = ['Group 1', 'Group 2']))\n",
    "    return X, y\n",
    "\n",
    "def train_SVM(X, y, C=1, scale = False, max_iter = 1000):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    clf = LinearSVC(C=C, max_iter=max_iter)\n",
    "    clf.fit(X,y)\n",
    "    return clf\n",
    "\n",
    "def print_accuracy(X_train, y_train, X_test, y_test, clf, scale=False):\n",
    "    if scale:\n",
    "        train_size = X_train.shape[0]\n",
    "        scaler = StandardScaler()\n",
    "        X = np.concatenate((X_train, X_test))\n",
    "        X = scaler.fit_transform(X)\n",
    "        X_train = X[:train_size,:]\n",
    "        X_test = X[train_size:,:] \n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    ac_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    ac_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "    display(pd.DataFrame(np.concatenate((ac_matrix_train,ac_matrix_test), axis =1), columns = [\"predicted G1 (training)\",\"predicted G2 (training)\", \"predicted G1 (test)\",\"predicted G2 (test)\"],index=[\"actual G1\",\"actual G2\"]))\n",
    "    \n",
    "def plot_pca_variance(X, ncomp = 1, scale = False):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    sns.set(style='darkgrid', context='talk')\n",
    "    plt.plot(np.arange(1,len(pca.explained_variance_ratio_)+1),np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    \n",
    "    plt.vlines(ncomp, 0, plt.gca().get_ylim()[1], color='r', linestyles = 'dashed')\n",
    "    h = np.cumsum(pca.explained_variance_ratio_)[ncomp -1]\n",
    "    plt.hlines(h, 0, plt.gca().get_xlim()[1], color='r', linestyles = 'dashed')\n",
    "    plt.title(str(ncomp) + ' components, ' + str(round(h, 3)) + ' variance explained')\n",
    "    plt.show()\n",
    "    \n",
    "def reduce_data(X, n, scale = False):\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=n)\n",
    "    Xr = pca.fit_transform(X)\n",
    "    return Xr\n",
    "\n",
    "def interact_split_data(Criteria, Group_1, Group_2):\n",
    "    global BRCA_X, BRCA_y\n",
    "    BRCA_X, BRCA_y = split_data(clinical_data, expression_data, Criteria, Group_1, Group_2)\n",
    "\n",
    "def interact_SVM_1(Rescale, Data_split, Max_iterations):\n",
    "    if Rescale:\n",
    "        X = scale_data(BRCA_X)\n",
    "    else:\n",
    "        X = BRCA_X\n",
    "    max_iter = int(Max_iterations)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BRCA_X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=1, scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf, scale = Rescale)\n",
    "    \n",
    "interact_gen(interact_split_data, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_SVM(interact_SVM_1, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Regularization\n",
    "\n",
    "You have learned about the propensity of machine learning methods to overfit.\n",
    "One way of trying to avoid overfitting is called [**regularization**](https://en.wikipedia.org/wiki/Regularization_(mathematics)).\n",
    "Regularization, as the names suggests, is a way of adding extra rules in a way that adds extra information on the model. It also serves as a great way to add **domain knowledge** into a general model.\n",
    "\n",
    "SVMs have a built in regularization mechanism in the form of the **C parameter**. If you are not familiar with this parameter, read a bit on it, and then play around with the SVM below.\n",
    "As with every form of regularization, it is more of an art than a science, and so a good value will depend a lot on how your data looks like.\n",
    "\n",
    "New interactive fields:\n",
    "* **C_parameter**: the value of the C parameter.\n",
    "\n",
    "## Questions\n",
    "\n",
    "\n",
    "* 4.1 What is the \"extra rule\" that regularization with C parameter adds to the algorithm?\n",
    "* 4.2 How would the trust you can have in labelling used for training correspond to the value of the C parameter you would choose? Think of hard to clasify/borderline, and provide one hypothetical example when a high C value would be preferabele, and one when a low one would be better.\n",
    "* 4.3 Did regularization with C parameter improve the performance of your predictor from part 3? Provide one reason why you think it was (not) so.\n",
    "* 4.4 Maximizing the perfomance of the classifier is not always the best criterium for deciding whether to regularize or not. Provide one hypothetical example when you would set a C value based on **prior knowledge** rather than on **best performance**. Hint: consider the data available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_SVM_2(Rescale, Data_split, Max_iterations, C_parameter):\n",
    "    max_iter = int(Max_iterations)\n",
    "    C = float(C_parameter)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BRCA_X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=C, scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf, scale = Rescale)\n",
    "\n",
    "interact_gen(interact_split_data, Criteria=Text('Surgical procedure first'), Group_1 = Text('Simple Mastectomy'), Group_2=Text('Lumpectomy'))\n",
    "interact_SVM(interact_SVM_2, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'), C_parameter = Text('1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Dimensionality\n",
    "\n",
    "Let's explore how the dimensionality of the data affects the prediction accuracy. You have heard of the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and, in bioinformatics, often we are subject to it just because of the amount of genes on a genome and the low number of samples.\n",
    "\n",
    "Luckily, you have also learned at least one way to reduce the dimensionality of the data. Here we will use PCA to reduce the dimensions from around 20 thousand genes (dimensions) to a more manageable number of dimensions.\n",
    "First, you will need to choose if you normalize all the dimensions before performing the PCA, then you will be shown how much of the variance in the data is captured by the first components (remember that the normalization also changes the variance). With this information you will be able to have an educated guess on how many principal components you want to use to train the SVM.\n",
    "\n",
    "The core PCA algorithm, as such, does not reduce the number of dimensions. It levarages co-variation of different variables (in different dimensions) to capture as much of total variations in as few dimensions, i.e. principal components, as possible. In contrast to dimensionality reduction methods such as UMAP or tSNE, no information is lost. Thus, given the full output matrices from PCA, it should be possible to retrive original values (check Singular Value Decomposition, e.g. in the lecture about PCA, for more details). After the transformation, the samples should vary a lot across the first few components, and little across all the rest. \n",
    "\n",
    "In most cases, PCA is used to retrive only a few first principal components, most often just the first two. The idea is that the first components should correspond to meaningful variance, driven by the the same (biological) processes in multiple samples, thus leading to co-variation. The other components can be, for some purposes, disregarded as \"noise\". By picking only few first components, PCA can be used for dimensionality reduction. That is very useful for visualization, as one can often show in 2D most of the variation corresponding to effects of key factors structuring the data. Another application can be to reduce the number of dimensions to avoid the curse of dimensionality, e.g. to improve the performance of an SVM-based predictor. This is what we will try to do in this exercise.\n",
    "\n",
    "As always, play with it as much as you need to answer the question.\n",
    "\n",
    "New interactive fields:\n",
    "\n",
    "* **PCA_scaling**: whether to apply the normalization before performing the PCA.\n",
    "* **N_components**: number of principal components selected.\n",
    "* **Reduce_dim**: whether to reduce the dimentions of the data before training the SVM.\n",
    "\n",
    "## Questions\n",
    "\n",
    "* 5.1 Explain one of the causes of **curse of dimensionality**. Why adding more information doesn't always increase the performance? It can be a characteristic of biological data/systems or a general property of the algorithms prone to the problem.\n",
    "* 5.2 After performing PCA on our data, how much of the variance did the first two components explain? What was the minimal number of components which explained most of the variation?\n",
    "* 5.3 Would you use the first two principal components to visualize the data? Give one reason why/why not.\n",
    "* 5.4 Did the dimensionality reduction improve the perfomance of the predictor? Provide parameters you have used and give the one cause you find most probable/influencial to explain the fact the performance did/did not improve? Could you have predicted the outcome based on the answers to p. 5.2 and the plot of explained variance vs the number of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_PCA_plot(PCA_scaling, N_components):\n",
    "    n_comp = int(N_components)\n",
    "    plot_pca_variance(BRCA_X, scale=PCA_scaling, ncomp = n_comp)\n",
    "    \n",
    "def interact_SVM_3(Rescale, Data_split, Max_iterations, C_parameter, Reduce_dim, PCA_scaling, N_components):\n",
    "    max_iter = int(Max_iterations)\n",
    "    n_comp = int(N_components)\n",
    "    C = float(C_parameter)\n",
    "    if Reduce_dim:\n",
    "        X = reduce_data(BRCA_X, n = n_comp, scale=PCA_scaling)\n",
    "    else:\n",
    "        X = BRCA_X\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, BRCA_y, test_size=Data_split)\n",
    "    clf = train_SVM(X_train, y_train, C=C,  scale = Rescale, max_iter=max_iter)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, clf,  scale = Rescale)\n",
    "    \n",
    "interact_gen(interact_split_data, Criteria=Text('ER Status By IHC'), Group_1 = Text('Positive'), Group_2=Text('Negative'))\n",
    "interact_plot(interact_PCA_plot, PCA_scaling = False, N_components = Text('1'))\n",
    "interact_SVM(interact_SVM_3, Rescale = False, Data_split = FloatSlider(min=0,max=1,value=0.1, step = 0.05), Max_iterations = Text('1000'), C_parameter = Text('1'), Reduce_dim = False, PCA_scaling = False, N_components = Text('1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "**Note**: To get full points in the **bonus part** you only have to answer 3 out of the 4 questions below, but you **have to specify** on your report which question you are leaving out.\n",
    "\n",
    "## ORA again\n",
    "* B1.1 Imagine again that organism with 10 genes (question 1.1), but now the pathway has only 5 genes, and all 3 differentially expressed genes belong to it. What is the p-value for the enrichment of that pathway? Show how you arrived at the result. If you use the hypergeometric test, explain what all the terms mean. (Hint: you may want to use the [binomial coefficient](https://en.wikipedia.org/wiki/Combination) to help you calculate)\n",
    "\n",
    "## Bias-variance trade-off\n",
    "\n",
    "Bias and variance are both sources of error in any predictive model. The bias-variance trade-off principle states that usually models with high bias error have low variance error, and vice-versa.\n",
    "\n",
    "* B2.1 Read more about **bias** and **variance**. Provide a one sentence explanation of each of the terms, in your own words.\n",
    "* B2.2 Imgagine a real-life example of a classifier. Explain what you would like to classify. Would high bias affect the perdictor's ability to infer the trait you are interested in for new data? (I.e. What kind of new datapoints/samples the predictor would be most likely to missclasify?) And how would it be affected by high variance.\n",
    "* B2.3 Explain why there is a trade-off between bias and variance in context of complexity and overfitting of a model. Use figures if think it's useful.\n",
    "* B2.4 Provide one way to control the trade-off in SVMs and one in clustering.\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "Some times you don't have enough data, and it is a shame to leave some of it out of the training set. One way to mitigate this is by using cross validation.\n",
    "\n",
    "* B3.1 Implement cross validation in one of the SVMs you used above. In how many parts you've divided your set? Why?\n",
    "* B3.2 Compare the results above with the results from previous classifiers in this lab. Do you achieve better performance?\n",
    "\n",
    "## Parameter optimization\n",
    "\n",
    "For the questions below, we will explore more in depth how some parameter affect the performance by doing [parameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
    "\n",
    "Perform a parameter optimization, using cross validation, for the following parameter on an SVM you used above. You can use ready functions such as [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to help. Plot and interpret the results for optimizing:\n",
    "\n",
    "* B4.1 Number of dimensions of the input (principal components)\n",
    "* B4.2 The C parameter\n",
    "* B4.3 The C parameter and number of dimensions, and scaling (of the input) at the same time.\n",
    "* B4.4 Comment about the relationship between C parameter and number of dimensions by using the results above.\n",
    "* B4.5 Now, investigate the claim that cross validation shouldn't be used to report performance. You may want to use [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/) to do so. In your own words, provide one argument for using cross validation to report performance, one against, and state your opinion on the matter. (i.e. Would you use cross validation to report performance in your own research?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
